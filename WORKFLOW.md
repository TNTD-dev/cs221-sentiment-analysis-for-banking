# ğŸ”„ Training Workflow Guide

This document provides a step-by-step guide for the complete workflow from local development to Google Colab training.

## ğŸ“‹ Quick Start Workflow

### 1ï¸âƒ£ Local Development & Preparation

```bash
# 1. Make sure your data is processed
# Check that these files exist:
ls data/processed/train_processed.csv
ls data/processed/val_processed.csv
ls data/processed/test_processed.csv

# 2. (Optional) Test training locally with baseline models
python train.py --model lr  # Quick test (< 1 minute)

# 3. Add all files to git
git add .
git commit -m "Add training scripts and processed data"
git push origin main
```

### 2ï¸âƒ£ Google Colab Setup

1. **Open Google Colab**: https://colab.research.google.com/

2. **Change Runtime to GPU**:
   - Click `Runtime` â†’ `Change runtime type`
   - Hardware accelerator: `T4 GPU`
   - Click `Save`

3. **Upload or Open Notebook**:
   - Option A: Upload `scripts/colab_train.ipynb`
   - Option B: Open from GitHub (File â†’ Open Notebook â†’ GitHub)

### 3ï¸âƒ£ Training on Colab

#### Update Configuration

In the notebook, find this cell and update:

```python
# ğŸ“ REPLACE THESE WITH YOUR ACTUAL VALUES
GITHUB_USERNAME = "YOUR_USERNAME"  # Your GitHub username
REPO_NAME = "YOUR_REPO_NAME"       # Your repository name
```

#### Run Training

Execute all cells in order:
1. âœ… Check GPU
2. âœ… (Optional) Mount Google Drive
3. âœ… Clone repository
4. âœ… Install dependencies
5. âœ… Verify data files
6. âœ… Train models (choose option A, B, C, or D)
7. âœ… View results
8. âœ… Download results
9. âœ… (Optional) Save to Google Drive

### 4ï¸âƒ£ Download & Analyze Results

After training completes:

1. **Download Results**: Two zip files will be downloaded
   - `trained_models.zip` - Contains all trained model weights
   - `training_results.zip` - Contains metrics, visualizations, and logs

2. **Extract Files**:
```bash
# On your local machine
unzip trained_models.zip -d models/
unzip training_results.zip -d results/
```

3. **View Results**:
```bash
# View comparison table
cat results/comparison.csv

# Open visualizations
open results/model_comparison.png
open results/confusion_matrix_phobert.png
```

## ğŸ¯ Training Options

### Option A: Train All Models (~45 minutes)
**Best for**: Complete comparison and final results

```python
!python train.py --model all --epochs 3 --batch-size 16
```

**You get**: All 5 models trained (LR, SVM, NB, PhoBERT, XLM-RoBERTa)

### Option B: Train Only PhoBERT (~20 minutes)
**Best for**: Quick high-performance model for Vietnamese

```python
!python train.py --model phobert --epochs 3 --batch-size 16
```

**You get**: Best performing model for Vietnamese sentiment analysis

### Option C: Train Only XLM-RoBERTa (~25 minutes)
**Best for**: Multilingual comparison

```python
!python train.py --model xlm-roberta --epochs 3 --batch-size 16
```

**You get**: Strong multilingual baseline

### Option D: Train Only Baseline Models (~2 minutes)
**Best for**: Quick testing and baseline comparison

```python
!python train.py --model lr
!python train.py --model svm
!python train.py --model nb
```

**You get**: Fast traditional ML baselines

## ğŸ”§ Customization

### Adjust Training Parameters

```python
# More epochs (better performance, longer training)
!python train.py --model phobert --epochs 5 --batch-size 16

# Larger batch size (faster training, more memory)
!python train.py --model phobert --epochs 3 --batch-size 32

# Smaller batch size (less memory, slower training)
!python train.py --model phobert --epochs 3 --batch-size 8
```

### Save to Google Drive

If you want results to persist:

```python
# Mount Drive first
from google.colab import drive
drive.mount('/content/drive')

# After training, copy results
import shutil
drive_dir = '/content/drive/MyDrive/sentiment_analysis_results'
shutil.copytree('models', f'{drive_dir}/models', dirs_exist_ok=True)
shutil.copytree('results', f'{drive_dir}/results', dirs_exist_ok=True)
```

## ğŸ“Š Understanding Results

### Metrics Explained

- **Accuracy**: Overall correct predictions / total predictions
- **Precision (Weighted)**: Accuracy weighted by class frequency
- **Recall (Weighted)**: Coverage weighted by class frequency
- **F1 (Weighted)**: Harmonic mean of precision and recall (weighted)
- **F1 (Macro)**: Average F1 across all classes (unweighted)

### Result Files

```
results/
â”œâ”€â”€ metrics.json              # All metrics in JSON format
â”œâ”€â”€ comparison.csv            # Easy-to-read comparison table
â”œâ”€â”€ model_comparison.png      # Visual comparison chart
â”œâ”€â”€ confusion_matrix_*.png    # Confusion matrix for each model
â””â”€â”€ training_logs.txt         # Detailed training logs
```

### Model Files

```
models/
â”œâ”€â”€ logistic_regression/
â”‚   â”œâ”€â”€ model.pkl             # Trained model
â”‚   â””â”€â”€ vectorizer.pkl        # TF-IDF vectorizer
â”œâ”€â”€ phobert/
â”‚   â”œâ”€â”€ pytorch_model.bin     # Model weights
â”‚   â”œâ”€â”€ config.json           # Model configuration
â”‚   â””â”€â”€ tokenizer_config.json # Tokenizer configuration
â””â”€â”€ ...
```

## âš¡ Tips & Best Practices

### For Faster Training
1. âœ… Use baseline models first to verify setup
2. âœ… Start with fewer epochs (--epochs 2)
3. âœ… Use larger batch sizes if GPU memory allows
4. âœ… Train one model at a time initially

### For Better Results
1. âœ… Use --epochs 5 for transformer models
2. âœ… Ensure GPU is enabled (check with first cell)
3. âœ… Monitor training logs for convergence
4. âœ… Compare multiple runs with different seeds

### Memory Management
- **T4 GPU has ~15GB**: Can handle batch_size up to 32
- **If OOM error**: Reduce batch_size to 8 or 16
- **For larger models**: Use gradient accumulation

### Troubleshooting

**Problem**: "No GPU found"
- **Solution**: Change runtime type to T4 GPU

**Problem**: "File not found" errors
- **Solution**: Check GitHub username and repo name are correct

**Problem**: "CUDA out of memory"
- **Solution**: Reduce batch size: `--batch-size 8`

**Problem**: Training is very slow
- **Solution**: Verify GPU is active, check GPU usage in Runtime menu

**Problem**: Repository not cloning
- **Solution**: Make sure repository is public or use authentication

## ğŸš€ Example Complete Workflow

```bash
# === LOCAL (Your Computer) ===
# 1. Verify data
ls data/processed/*.csv

# 2. Test locally (optional)
python train.py --model lr

# 3. Push to GitHub
git add .
git commit -m "Ready for training"
git push origin main

# === GOOGLE COLAB ===
# 4. Open colab_train.ipynb
# 5. Update GitHub username and repo name
# 6. Runtime â†’ Change runtime â†’ T4 GPU
# 7. Run all cells
# 8. Download results when done

# === LOCAL (After Training) ===
# 9. Extract results
unzip trained_models.zip -d models/
unzip training_results.zip -d results/

# 10. View comparison
cat results/comparison.csv
```

## ğŸ“š Additional Resources

- **Hugging Face PhoBERT**: https://huggingface.co/vinai/phobert-base
- **Google Colab Guide**: https://colab.research.google.com/notebooks/intro.ipynb
- **Transformers Documentation**: https://huggingface.co/docs/transformers/

---

Need help? Open an issue on GitHub or check the main README.md for more details.

